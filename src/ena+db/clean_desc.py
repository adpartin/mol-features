"""
This code takes the original descriptors from ena+db.desc,
cleans data, and saves. The original file is not overwritten.
These descriptors were generated by Rick.

Note that for the big runs we don't remove descriptor cols even
if all values are NaNs (we simply impute with 0).
"""
import warnings
warnings.filterwarnings('ignore')

import os
import sys
from pathlib import Path
from time import time

import numpy as np
import pandas as pd

filepath = Path(__file__).resolve().parent

# Utils
sys.path.append( os.path.abspath(filepath/'../utils') )
# from utils.classlogger import Logger
# from utils.utils import load_data, drop_dup_rows, dropna
from classlogger import Logger
from utils import load_data, drop_dup_rows, dropna

datadir = Path('/vol/ml/apartin/projects/covid-19/ML-docking-dataframe-generator/data/raw/ena+db')
# outdir  = Path('/vol/ml/apartin/projects/covid-19/ML-docking-dataframe-generator/data/processed/descriptors')
# outdir  = Path('/vol/ml/apartin/projects/covid-19/ML-docking-dataframe-generator/data/processed/descriptors/ena+db')
outdir  = Path('/vol/ml/apartin/projects/covid-19/ML-docking-dataframe-generator/data/processed/features/ena+db')
os.makedirs(outdir, exist_ok=True)


t0 = time()
lg = Logger( outdir/'clean.desc.log' )
print_fn = lg.logger.info

print_fn('\nPython filepath   {}'.format( filepath ))
print_fn('Original data dir {}'.format( datadir ))
print_fn('Output data dir   {}'.format( outdir ))

# File names
in_fname = 'ena+db.desc'
in_fpath  = datadir/in_fname
out_fpath = outdir/in_fname

# Load Modred descriptors (this descriptors are the result
# from mordred run on ena+db.can from Rick)
print_fn('\nLoad {}'.format( in_fpath ))
dsc = pd.read_csv(in_fpath)
print_fn(dsc.shape)

# Drop duplicates
dsc = drop_dup_rows(dsc, print_fn=print_fn)
print_fn(dsc.iloc[:5, :7])

# Filter NaN values - step 1
# Drop rows where all values are NaNs
print_fn('\nDrop rows where all values are NaN ...')
print_fn('Shape: {}'.format( dsc.shape ))
idx = (dsc.isna().sum(axis=1)==dsc.shape[1]).values
dsc = dsc.iloc[~idx, :]
# Drop cols where all values are NaNs
# idx = (dsc.isna().sum(axis=0)==dsc.shape[0]).values
# dsc = dsc.iloc[:, ~idx]
print_fn('Shape: {}'.format( dsc.shape ))

# Filter rows/cols based on NaN values - step 2
# Remove rows and cols based on a thershold of NaN values
# print(dsc.isna().sum(axis=1).sort_values(ascending=False))
# p=dsc.isna().sum(axis=1).sort_values(ascending=False).hist(bins=100);
th = 0.2
print_fn('\nDrop rows (drugs) with at least {} NaNs (at least {} out of {}).'.format(
    th, int(th * dsc.shape[1]), dsc.shape[1]))
print_fn('Shape: {}'.format( dsc.shape ))
dsc = dropna(dsc, axis=1, th=th)
print_fn('Shape: {}'.format( dsc.shape ))

## print(dsc.isna().sum(axis=0).sort_values(ascending=False))
## p=dsc.isna().sum(axis=0).sort_values(ascending=False).hist(bins=100); # print_fn('Drop cols (drugs) with at least {} NaN values (at least {} out of {}).'.format(
#     th, int(th * dsc.shape[0]), dsc.shape[0]))
# th = 0.1
# cnt0 = dsc.shape[1]; print_fn('Count: {}'.format( cnt0 ))
# dsc = dropna(dsc, axis=0, th=th)
# cnt1 = dsc.shape[1]; print_fn('Count: {}'.format( cnt1 ));
# print_fn('Dropped duplicates: {}'.format( cnt0-cnt1 ))

# Cast columns (descriptors)
print_fn('\nCast descriptors to float ...')
dsc = dsc.astype({c: np.float32 for c in dsc.columns[1:]})

# Impute missing values
print_fn('\nImpute NaNs ...')
# dsc = dsc.reset_index(drop=True)
# dsc = impute_values(dsc, print_fn=print_fn) # ap's approach
dsc = dsc.fillna(0.0)
print_fn('Total NaNs: {}.'.format( dsc.isna().values.flatten().sum() ))

# Prefix desc names
dsc.columns = ['name'] + ['mod.'+c for c in dsc.columns[1:]]

# Save
print_fn('\nSave ...')
print_fn( dsc.shape )
dsc = dsc.reset_index(drop=True)
dsc.to_parquet( str(out_fpath) + '.parquet' )

print_fn('\nRuntime {:.2f} mins'.format( (time()-t0)/60 ))
print_fn('Done.')
lg.kill_logger()


